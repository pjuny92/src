{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 첫 수업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 처음에 프롬프트에 pip install pyspark를 설치한 이후에 밑에 실행할 것!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing p_spark_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile p_spark_2.py\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "spark_home='C:/Users/JUNYONG/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "findspark.init(spark_home)\n",
    "\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf=pyspark.SparkConf().setAppName(\"myApp\")\n",
    "sc=pyspark.SparkContext(conf=conf)\n",
    "\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* C:\\Users\\JUNYONG\\Downloads\\spark-1.6.0-bin-hadoop2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 현재 설치되어 있는 곳 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_home='C:/Users/JUNYONG/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "findspark.init(spark_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/JUNYONG/Downloads/spark-1.6.0-bin-hadoop2.6\n"
     ]
    }
   ],
   "source": [
    "print spark_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* conf 디렉토리 밑에 있는 것을 읽어오는 명령어\n",
    "* SparkContext는 클라이언트를 만드는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf=pyspark.SparkConf().setAppName(\"myApp\")\n",
    "sc=pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.app.name', u'myApp'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.master', u'local[*]'),\n",
       " (u'spark.submit.deployMode', u'client')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c=list([39.2, 36.5, 37.3, 37.0])\n",
    "\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for x in c :\n",
    "        _c=9./5*x+32;\n",
    "        f.append(_c)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 98.60000000000001]\n"
     ]
    }
   ],
   "source": [
    "print c2f(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-676db02f860b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mc2f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m9.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc2f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-676db02f860b>\u001b[0m in \u001b[0;36mc2f\u001b[1;34m(c)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mc2f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m9.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc2f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "c=list([39.2, 36.5, 37.3, 37.0])\n",
    "\n",
    "def c2f(c):\n",
    "    return (9./5*x+32)\n",
    "map(c2f, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ProjectRoot/data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ProjectRoot/data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Originally developed at the University of California, Berkeley's AMPLab, \n",
    "the Spark codebase was later donated to the Apache Software Foundation, \n",
    "which has maintained it since. \n",
    "Spark provides an interface for programming entire clusters with \n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile=sc.textFile(\"ProjectRoot/data/ds_spark_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tokenization 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words=textFile.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark'],\n",
       " [u'Originally',\n",
       "  u'developed',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'California,',\n",
       "  u\"Berkeley's\",\n",
       "  u'AMPLab,',\n",
       "  u''],\n",
       " [u'the',\n",
       "  u'Spark',\n",
       "  u'codebase',\n",
       "  u'was',\n",
       "  u'later',\n",
       "  u'donated',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'Apache',\n",
       "  u'Software',\n",
       "  u'Foundation,',\n",
       "  u''],\n",
       " [u'which', u'has', u'maintained', u'it', u'since.', u''],\n",
       " [u'Spark',\n",
       "  u'provides',\n",
       "  u'an',\n",
       "  u'interface',\n",
       "  u'for',\n",
       "  u'programming',\n",
       "  u'entire',\n",
       "  u'clusters',\n",
       "  u'with',\n",
       "  u''],\n",
       " [u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 73, 72, 31, 65, 46]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile\\\n",
    "    .map(lambda x:len(x))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_sparkLine=textFile\\\n",
    "    .filter(lambda line:\"Spark\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sparkLine.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_sparkLine=textFile\\\n",
    "    .filter(lambda line:u\"스파크\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sparkLine.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "print _sparkLine.collect()\n",
    "print _sparkLine.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=[1,2,3]\n",
    "myrdd=sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squared=myrdd.map(lambda x:x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[\"this is a line\", \"this is another line\"]\n",
    "myrdd=sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=myrdd.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'line'], ['this', 'is', 'another', 'line']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is AA line', 'this is AAnother line']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.map(lambda x:x.replace(\"a\",\"AA\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ProjectRoot/data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ProjectRoot/data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=sc.textFile(\"ProjectRoot/data/ds_spark_2cols.csv\")\n",
    "csvRdd=f.map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1'],\n",
       " [u'14', u' 19'],\n",
       " [u'46', u' 1'],\n",
       " [u'10', u' 34'],\n",
       " [u'28', u' 3'],\n",
       " [u'48', u' 1'],\n",
       " [u'16', u' 2'],\n",
       " [u'30', u' 3'],\n",
       " [u'32', u' 2'],\n",
       " [u'48', u' 1'],\n",
       " [u'31', u' 2'],\n",
       " [u'22', u' 1'],\n",
       " [u'12', u' 3'],\n",
       " [u'39', u' 29'],\n",
       " [u'19', u' 37'],\n",
       " [u'25', u' 2']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.0,3.0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "dv1=Vectors.dense([1,2,3])\n",
    "print dv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 2.0, 3.0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dv2 = np.array([1, 2, 3])\n",
    "Vectors.dense(dv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  3.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "sv1.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "LabeledPoint(1.0, Vectors.dense([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDf=sqlCtx.createDataFrame([\n",
    "        (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "        (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "        (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    ],['label','features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이것이 data frame이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|label|     features|\n",
      "+-----+-------------+\n",
      "|  1.0|[0.0,1.1,0.1]|\n",
      "|  1.0|[0.0,1.1,0.1]|\n",
      "|  1.0|[0.0,1.1,0.1]|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스키마는 모델이라 보아야한다. 머신러닝에 값을 넣어줄 수 있는 형태로 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = sc.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델이라고도 한다 schema는 label은 더블타입 features는 벡터로 정의한다. null 값을 가질 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scheme를 적용해서 trainDf라는 data frame을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x8816b00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmfn = \"C:\\Users\\JUNYONG\\Downloads\\spark-1.6.0-bin-hadoop2.6\\data\\mllib\\sample_libsvm_data.txt\"\n",
    "svmDf = sqlCtx.read.format(\"libsvm\").load(svmfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ProjectRoot/data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ProjectRoot/data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "Originally developed at the University of California, Berkeley's AMPLab, \n",
    "the Spark codebase was later donated to the Apache Software Foundation, \n",
    "which has maintained it since. \n",
    "Spark provides an interface for programming entire clusters with \n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 저장한 위키피디아를 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"ProjectRoot/data/ds_spark_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc=lines.flatMap(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc2=lines.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark',\n",
       "  u'Apache',\n",
       "  u'Spark'],\n",
       " [u'Originally',\n",
       "  u'developed',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'California,',\n",
       "  u\"Berkeley's\",\n",
       "  u'AMPLab,'],\n",
       " [u'the',\n",
       "  u'Spark',\n",
       "  u'codebase',\n",
       "  u'was',\n",
       "  u'later',\n",
       "  u'donated',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'Apache',\n",
       "  u'Software',\n",
       "  u'Foundation,'],\n",
       " [u'which', u'has', u'maintained', u'it', u'since.'],\n",
       " [u'Spark',\n",
       "  u'provides',\n",
       "  u'an',\n",
       "  u'interface',\n",
       "  u'for',\n",
       "  u'programming',\n",
       "  u'entire',\n",
       "  u'clusters',\n",
       "  u'with'],\n",
       " [u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'an',\n",
       " u'open',\n",
       " u'source',\n",
       " u'cluster',\n",
       " u'computing',\n",
       " u'framework.',\n",
       " u'\\uc544\\ud30c\\uce58',\n",
       " u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       " u'\\uc624\\ud508',\n",
       " u'\\uc18c\\uc2a4',\n",
       " u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       " u'\\ucef4\\ud4e8\\ud305',\n",
       " u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Originally',\n",
       " u'developed',\n",
       " u'at',\n",
       " u'the',\n",
       " u'University',\n",
       " u'of',\n",
       " u'California,',\n",
       " u\"Berkeley's\",\n",
       " u'AMPLab,',\n",
       " u'the',\n",
       " u'Spark',\n",
       " u'codebase',\n",
       " u'was',\n",
       " u'later',\n",
       " u'donated',\n",
       " u'to',\n",
       " u'the',\n",
       " u'Apache',\n",
       " u'Software',\n",
       " u'Foundation,',\n",
       " u'which',\n",
       " u'has',\n",
       " u'maintained',\n",
       " u'it',\n",
       " u'since.',\n",
       " u'Spark',\n",
       " u'provides',\n",
       " u'an',\n",
       " u'interface',\n",
       " u'for',\n",
       " u'programming',\n",
       " u'entire',\n",
       " u'clusters',\n",
       " u'with',\n",
       " u'implicit',\n",
       " u'data',\n",
       " u'parallelism',\n",
       " u'and',\n",
       " u'fault-tolerance.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = sc.textFile(\"ProjectRoot/data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .map(lambda x:[(i,1) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'wikipedia', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'is', 1), (u'an', 1), (u'open', 1), (u'source', 1), (u'cluster', 1), (u'computing', 1), (u'framework', 1)]\n",
      "[(u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1), (u'\\uc624\\ud508', 1), (u'\\uc18c\\uc2a4', 1), (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1), (u'\\ucef4\\ud4e8\\ud305', 1), (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1)]\n",
      "[(u'originally', 1), (u'developed', 1), (u'at', 1), (u'the', 1), (u'university', 1), (u'of', 1), (u'california', 1), (u\"berkeley's\", 1), (u'amplab', 1)]\n",
      "[(u'the', 1), (u'spark', 1), (u'codebase', 1), (u'was', 1), (u'later', 1), (u'donated', 1), (u'to', 1), (u'the', 1), (u'apache', 1), (u'software', 1), (u'foundation', 1)]\n",
      "[(u'which', 1), (u'has', 1), (u'maintained', 1), (u'it', 1), (u'since', 1)]\n",
      "[(u'spark', 1), (u'provides', 1), (u'an', 1), (u'interface', 1), (u'for', 1), (u'programming', 1), (u'entire', 1), (u'clusters', 1), (u'with', 1)]\n",
      "[(u'implicit', 1), (u'data', 1), (u'parallelism', 1), (u'and', 1), (u'fault', 1), (u'tolerance', 1)]\n"
     ]
    }
   ],
   "source": [
    "for e in wc.collect():\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pDf=sqlCtx.read.json(\"C:/Users\\JUNYONG/Downloads\\spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pDf.show()\n",
    "pDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 조건에 맞게 필터한다. age가 21이상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pDf.filter(pDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 메모리에 테이블로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pDf.registerTempTable(\"people\")\n",
    "sqlCtx.sql(\"select name from people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', u'ClubCountry': u'Argentina', u'Year': 1930, u'Number': u'', u'Competition': u'World Cup', u'DateOfBirth': u'1905-5-5', u'Team': u'Argentina', u'Position': u'GK', u'FullName': u'\\xc3ngel Bossio', u'IsCaptain': False}\n"
     ]
    }
   ],
   "source": [
    "print wc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wcDf=sqlCtx.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "|Quilmes AtlÃ©tico...|Argentina|1930|\n",
      "|          Boca Junio|Argentina|1930|\n",
      "|Central Norte TucumÃ|Argentina|1930|\n",
      "|Club Atletico Est...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.registerTempTable(\"wc\")\n",
    "sqlCtx.sql(\"select Club,Team,Year from wc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.05 S-9 정량데이터분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 첫째줄이 Class , Label 이진분류 , 나머지 네개는 features\n",
    "* binary 분류를 하는 것이다\n",
    "* 명목변수란? 데이터간의 순서가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile p_spark_2.py\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "spark_home='C:/Users/JUNYONG/Downloads/spark-1.6.0-bin-hadoop2.6'\n",
    "findspark.init(spark_home)\n",
    "\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf=pyspark.SparkConf().setAppName(\"myApp\")\n",
    "sc=pyspark.SparkContext(conf=conf)\n",
    "\n",
    "sc._conf.getAll()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.createDataFrame(\n",
    "    [\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'true', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'good'],\n",
    "        ['Yes','middle', 'true', 'true', 'good'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'excellent'],\n",
    "        ['No','old', 'false', 'false', 'fair'],\n",
    "    ],\n",
    "    ['cls','age','f1','f2','f3']\n",
    ")\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"labels\")\n",
    "model=labelIndexer.fit(df)\n",
    "df1=model.transform(df)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"age\", outputCol=\"att1\")\n",
    "model=labelIndexer.fit(df1)\n",
    "df2=model.transform(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.createDataFrame(\n",
    "    [\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'true', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'good'],\n",
    "        ['Yes','middle', 'true', 'true', 'good'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'excellent'],\n",
    "        ['No','old', 'false', 'false', 'fair'],\n",
    "    ],\n",
    "    ['cls','age','f1','f2','f3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"labels\")\n",
    "model=labelIndexer.fit(df)\n",
    "df1=model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+---------+------+\n",
      "|cls|   age|   f1|   f2|       f3|labels|\n",
      "+---+------+-----+-----+---------+------+\n",
      "| No| young|false|false|     fair|   1.0|\n",
      "| No| young|false|false|     good|   1.0|\n",
      "|Yes| young| true|false|     good|   0.0|\n",
      "|Yes| young| true| true|     fair|   0.0|\n",
      "| No| young|false|false|     fair|   1.0|\n",
      "| No|middle|false|false|     fair|   1.0|\n",
      "| No|middle|false|false|     good|   1.0|\n",
      "|Yes|middle| true| true|     good|   0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0|\n",
      "|Yes|   old|false| true|excellent|   0.0|\n",
      "|Yes|   old|false| true|     good|   0.0|\n",
      "|Yes|   old| true|false|     good|   0.0|\n",
      "|Yes|   old| true|false|excellent|   0.0|\n",
      "| No|   old|false|false|     fair|   1.0|\n",
      "+---+------+-----+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"age\", outputCol=\"att1\")\n",
    "model=labelIndexer.fit(df1)\n",
    "df2=model.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f1\", outputCol=\"att2\")\n",
    "model=labelIndexer.fit(df2)\n",
    "df3=model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f2\", outputCol=\"att3\")\n",
    "model=labelIndexer.fit(df3)\n",
    "df4=model.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f3\", outputCol=\"att4\")\n",
    "model=labelIndexer.fit(df4)\n",
    "df5=model.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "|cls|   age|   f1|   f2|       f3|labels|att1|att2|att3|att4|\n",
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "| No| young|false|false|     fair|   1.0| 0.0| 0.0| 0.0| 1.0|\n",
      "| No| young|false|false|     good|   1.0| 0.0| 0.0| 0.0| 0.0|\n",
      "|Yes| young| true|false|     good|   0.0| 0.0| 1.0| 0.0| 0.0|\n",
      "|Yes| young| true| true|     fair|   0.0| 0.0| 1.0| 1.0| 1.0|\n",
      "| No| young|false|false|     fair|   1.0| 0.0| 0.0| 0.0| 1.0|\n",
      "| No|middle|false|false|     fair|   1.0| 1.0| 0.0| 0.0| 1.0|\n",
      "| No|middle|false|false|     good|   1.0| 1.0| 0.0| 0.0| 0.0|\n",
      "|Yes|middle| true| true|     good|   0.0| 1.0| 1.0| 1.0| 0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0| 1.0| 0.0| 1.0| 2.0|\n",
      "|Yes|middle|false| true|excellent|   0.0| 1.0| 0.0| 1.0| 2.0|\n",
      "|Yes|   old|false| true|excellent|   0.0| 2.0| 0.0| 1.0| 2.0|\n",
      "|Yes|   old|false| true|     good|   0.0| 2.0| 0.0| 1.0| 0.0|\n",
      "|Yes|   old| true|false|     good|   0.0| 2.0| 1.0| 0.0| 0.0|\n",
      "|Yes|   old| true|false|excellent|   0.0| 2.0| 1.0| 0.0| 2.0|\n",
      "| No|   old|false|false|     fair|   1.0| 2.0| 0.0| 0.0| 1.0|\n",
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"att1\",\"att2\",\"att3\",\"att4\"],outputCol=\"features\")\n",
    "df6 = va.transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df7=df6.withColumnRenamed('labels','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- att1: double (nullable = true)\n",
      " |-- att2: double (nullable = true)\n",
      " |-- att3: double (nullable = true)\n",
      " |-- att4: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+---------+-----+----+----+----+----+-----------------+\n",
      "|cls|   age|   f1|   f2|       f3|label|att1|att2|att3|att4|         features|\n",
      "+---+------+-----+-----+---------+-----+----+----+----+----+-----------------+\n",
      "| No| young|false|false|     fair|  1.0| 0.0| 0.0| 0.0| 1.0|    (4,[3],[1.0])|\n",
      "| No| young|false|false|     good|  1.0| 0.0| 0.0| 0.0| 0.0|        (4,[],[])|\n",
      "|Yes| young| true|false|     good|  0.0| 0.0| 1.0| 0.0| 0.0|    (4,[1],[1.0])|\n",
      "|Yes| young| true| true|     fair|  0.0| 0.0| 1.0| 1.0| 1.0|[0.0,1.0,1.0,1.0]|\n",
      "| No| young|false|false|     fair|  1.0| 0.0| 0.0| 0.0| 1.0|    (4,[3],[1.0])|\n",
      "| No|middle|false|false|     fair|  1.0| 1.0| 0.0| 0.0| 1.0|[1.0,0.0,0.0,1.0]|\n",
      "| No|middle|false|false|     good|  1.0| 1.0| 0.0| 0.0| 0.0|    (4,[0],[1.0])|\n",
      "|Yes|middle| true| true|     good|  0.0| 1.0| 1.0| 1.0| 0.0|[1.0,1.0,1.0,0.0]|\n",
      "|Yes|middle|false| true|excellent|  0.0| 1.0| 0.0| 1.0| 2.0|[1.0,0.0,1.0,2.0]|\n",
      "|Yes|middle|false| true|excellent|  0.0| 1.0| 0.0| 1.0| 2.0|[1.0,0.0,1.0,2.0]|\n",
      "|Yes|   old|false| true|excellent|  0.0| 2.0| 0.0| 1.0| 2.0|[2.0,0.0,1.0,2.0]|\n",
      "|Yes|   old|false| true|     good|  0.0| 2.0| 0.0| 1.0| 0.0|[2.0,0.0,1.0,0.0]|\n",
      "|Yes|   old| true|false|     good|  0.0| 2.0| 1.0| 0.0| 0.0|[2.0,1.0,0.0,0.0]|\n",
      "|Yes|   old| true|false|excellent|  0.0| 2.0| 1.0| 0.0| 2.0|[2.0,1.0,0.0,2.0]|\n",
      "| No|   old|false|false|     fair|  1.0| 2.0| 0.0| 0.0| 1.0|[2.0,0.0,0.0,1.0]|\n",
      "+---+------+-----+-----+---------+-----+----+----+----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* select를 하면 데이터 slicing을 한다. select하는 속성명을 따옴표로 적어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=df7.select('label','features')\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|        (4,[],[])|\n",
      "|  0.0|    (4,[1],[1.0])|\n",
      "|  0.0|[0.0,1.0,1.0,1.0]|\n",
      "|  1.0|    (4,[3],[1.0])|\n",
      "|  1.0|[1.0,0.0,0.0,1.0]|\n",
      "|  1.0|    (4,[0],[1.0])|\n",
      "|  0.0|[1.0,1.0,1.0,0.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[1.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,2.0]|\n",
      "|  0.0|[2.0,0.0,1.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,0.0]|\n",
      "|  0.0|[2.0,1.0,0.0,2.0]|\n",
      "|  1.0|[2.0,0.0,0.0,1.0]|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "trainRdd = trainDf.map(lambda row: LabeledPoint(row.label,row.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, (4,[3],[1.0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression ML 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.50705810019,-5.31916107407,-5.04694958332,-0.351455356638]\n",
      "3.2822908185\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "model1 = lr.fit(trainDf)\n",
    "print model1.coefficients\n",
    "print model1.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "test0 = sc.parallelize([Row(features=Vectors.dense(2.0,0.0,0.0,1.0))]).toDF()\n",
    "result = model1.transform(test0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.createDataFrame(\n",
    "    [\n",
    "        [0,'my dog has flea problems. help please.'],\n",
    "        [1,'maybe not take him to dog park stupid'],\n",
    "        [0,'my dalmation is so cute. I love him'],\n",
    "        [1,'stop posting stupid worthless garbage'],\n",
    "        [0,'mr licks ate my steak how to stop him'],\n",
    "        [1,'quit buying worthless dog food stupid'],\n",
    "        [0,u'우리 강아지 벌레 있어요 도와주세요'],\n",
    "        [0,u'우리 강아지 귀여워 너 사랑해'],\n",
    "        [1,u'강아지 공원 가지마 바보같이'],\n",
    "        [1,u'강아지 음식 구매 마세요 바보같이']\n",
    "    ],\n",
    "    ['cls','sent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: long (nullable = true)\n",
      " |-- sent: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(cls=0, sent=u'my dog has flea problems. help please.')\n",
      "Row(cls=1, sent=u'maybe not take him to dog park stupid')\n",
      "Row(cls=0, sent=u'my dalmation is so cute. I love him')\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"cls\", \"sent\").take(3):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "|cls|                sent|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|my dog has flea p...|[my, dog, has, fl...|\n",
      "|  1|maybe not take hi...|[maybe, not, take...|\n",
      "|  0|my dalmation is s...|[my, dalmation, i...|\n",
      "|  1|stop posting stup...|[stop, posting, s...|\n",
      "|  0|mr licks ate my s...|[mr, licks, ate, ...|\n",
      "|  1|quit buying worth...|[quit, buying, wo...|\n",
      "|  0| 우리 강아지 벌레 있어요 도와주세요|[우리, 강아지, 벌레, 있어요...|\n",
      "|  0|    우리 강아지 귀여워 너 사랑해|[우리, 강아지, 귀여워, 너,...|\n",
      "|  1|     강아지 공원 가지마 바보같이|[강아지, 공원, 가지마, 바보같이]|\n",
      "|  1|  강아지 음식 구매 마세요 바보같이|[강아지, 음식, 구매, 마세요...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_4cc3be537d51b633f572"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a about above across after afterwards again against all almost alone along already also although always am among amongst amoungst amount an and another any anyhow anyone anything anyway anywhere are around as at back be became because become becomes becoming been before beforehand behind being below beside besides between beyond bill both bottom but by call can cannot cant co con could couldnt cry de describe detail do done down due during each eg eight either eleven else elsewhere empty enough etc even ever every everyone everything everywhere except few fifteen fify fill find fire first five for former formerly forty found four from front full further get give go had has hasnt have he hence her here hereafter hereby herein hereupon hers herself him himself his how however hundred i ie if in inc indeed interest into is it its itself keep last latter latterly least less ltd made many may me meanwhile might mill mine more moreover most mostly move much must my myself name namely neither never nevertheless next nine no nobody none noone nor not nothing now nowhere of off often on once one only onto or other others otherwise our ours ourselves out over own part per perhaps please put rather re same see seem seemed seeming seems serious several she should show side since sincere six sixty so some somehow someone something sometime sometimes somewhere still such system take ten than that the their them themselves then thence there thereafter thereby therefore therein thereupon these they thick thin third this those though three through throughout thru thus to together too top toward towards twelve twenty two un under until up upon us very via was we well were what whatever when whence whenever where whereafter whereas whereby wherein whereupon wherever whether which while whither who whoever whole whom whose why will with within without would yet you your yours yourself yourselves 나 너 우리\n"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|cls|                sent|               words|             nostops|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|my dog has flea p...|[my, dog, has, fl...|[dog, flea, probl...|\n",
      "|  1|maybe not take hi...|[maybe, not, take...|[maybe, dog, park...|\n",
      "|  0|my dalmation is s...|[my, dalmation, i...|[dalmation, cute....|\n",
      "|  1|stop posting stup...|[stop, posting, s...|[stop, posting, s...|\n",
      "|  0|mr licks ate my s...|[mr, licks, ate, ...|[mr, licks, ate, ...|\n",
      "|  1|quit buying worth...|[quit, buying, wo...|[quit, buying, wo...|\n",
      "|  0| 우리 강아지 벌레 있어요 도와주세요|[우리, 강아지, 벌레, 있어요...|[강아지, 벌레, 있어요, 도와...|\n",
      "|  0|    우리 강아지 귀여워 너 사랑해|[우리, 강아지, 귀여워, 너,...|     [강아지, 귀여워, 사랑해]|\n",
      "|  1|     강아지 공원 가지마 바보같이|[강아지, 공원, 가지마, 바보같이]|[강아지, 공원, 가지마, 바보같이]|\n",
      "|  1|  강아지 음식 구매 마세요 바보같이|[강아지, 음식, 구매, 마세요...|[강아지, 음식, 구매, 마세요...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(tokDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               words|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[my, dog, has, fl...|[dog, flea, probl...|(30,[1,9,17,24,26...|\n",
      "|[maybe, not, take...|[maybe, dog, park...|(30,[1,2,11,20],[...|\n",
      "|[my, dalmation, i...|[dalmation, cute....|(30,[21,29],[1.0,...|\n",
      "|[stop, posting, s...|[stop, posting, s...|(30,[2,3,4,16,28]...|\n",
      "|[mr, licks, ate, ...|[mr, licks, ate, ...|(30,[3,18,22,23,2...|\n",
      "|[quit, buying, wo...|[quit, buying, wo...|(30,[1,2,4,8,25],...|\n",
      "|[우리, 강아지, 벌레, 있어요...|[강아지, 벌레, 있어요, 도와...|(30,[0,12,13,15],...|\n",
      "|[우리, 강아지, 귀여워, 너,...|     [강아지, 귀여워, 사랑해]|(30,[0,10],[1.0,1...|\n",
      "|[강아지, 공원, 가지마, 바보같이]|[강아지, 공원, 가지마, 바보같이]|(30,[0,5,14],[1.0...|\n",
      "|[강아지, 음식, 구매, 마세요...|[강아지, 음식, 구매, 마세요...|(30,[0,5,6,7,19],...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30,minDF=1.0)\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "\n",
    "cvDf.collect()\n",
    "cvDf.select('words','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cv=SparseVector(30, {1: 1.0, 9: 1.0, 17: 1.0, 24: 1.0, 26: 1.0})),\n",
       " Row(cv=SparseVector(30, {1: 1.0, 2: 1.0, 11: 1.0, 20: 1.0})),\n",
       " Row(cv=SparseVector(30, {21: 1.0, 29: 1.0})),\n",
       " Row(cv=SparseVector(30, {2: 1.0, 3: 1.0, 4: 1.0, 16: 1.0, 28: 1.0})),\n",
       " Row(cv=SparseVector(30, {3: 1.0, 18: 1.0, 22: 1.0, 23: 1.0, 27: 1.0})),\n",
       " Row(cv=SparseVector(30, {1: 1.0, 2: 1.0, 4: 1.0, 8: 1.0, 25: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 12: 1.0, 13: 1.0, 15: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 10: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 5: 1.0, 14: 1.0})),\n",
       " Row(cv=SparseVector(30, {0: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 19: 1.0}))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvDf.select('cv').take(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지 dog stupid stop worthless 바보같이 마세요 구매 buying please. 귀여워 maybe 도와주세요 벌레 가지마 있어요 garbage help mr 음식 park love ate steak problems. food flea licks posting cute.\n"
     ]
    }
   ],
   "source": [
    "for v in cvModel.vocabulary:\n",
    "    print v,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"labels\")\n",
    "model=labelIndexer.fit(cvDf)\n",
    "trainDf2=model.transform(cvDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_spark_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_hello.py\n",
    "print \"---------BEGIN-----------\"\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName1\")\n",
    "sc   = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print \"---------RESULT-----------\"\n",
    "print sc\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "print \"mean=\",rdd.mean()\n",
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "for num in squared:\n",
    "    print \"%i \" % (num)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
